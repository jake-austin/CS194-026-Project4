{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18d2cd9-5258-4626-83bb-bf23337fb6df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Base Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f75232d-f814-429e-ae8f-5840ec75bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from cv2 import resize\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "import skimage.io as skio\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd2bd2-7c5d-421d-aa36-6eb6758b32f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14926414-9235-4722-baef-a74eef025c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    im = plt.imread(path)\n",
    "    im = im.astype(float)\n",
    "    return im\n",
    "\n",
    "def show(im, figsize = 10, cmap=None):\n",
    "    plt.figure(figsize=(figsize,figsize))\n",
    "    plt.imshow(im, cmap=cmap)\n",
    "    \n",
    "def resize_preserve(img, shape):\n",
    "    \"\"\"\n",
    "    Here, this is to crop without warping the input image\n",
    "    \"\"\"\n",
    "    cv2_shape = np.array([img.shape[1], img.shape[0]])\n",
    "    initial_upscale = max(shape[0]/cv2_shape[0], shape[1]/cv2_shape[1])\n",
    "    img = resize(img, (cv2_shape * initial_upscale).astype(int))\n",
    "    cv2_shape = np.array([img.shape[1], img.shape[0]])\n",
    "\n",
    "    # Crop in x direction if needed\n",
    "    xdiff = img.shape[1] - shape[0]\n",
    "    if xdiff:\n",
    "        img = img[:,xdiff//2:img.shape[1]-xdiff//2]\n",
    "    \n",
    "    # Crop in y direction if needed\n",
    "    ydiff = img.shape[0] - shape[1]\n",
    "    if ydiff:\n",
    "        img = img[ydiff//2:img.shape[0]-ydiff//2]\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da35001-d31c-4669-a702-0a85e6681c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1d888-9f29-4961-8078-b4db98ff0af4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb80ca2a-11ce-4236-809a-6ae2e08c0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f8f64-d152-48f2-a469-05420aca2987",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b75544e-9be4-4610-af92-e2efaa24ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondences(images, dims = (1, 1), timeout = -1, include_edges = False, cmap = None, mesh = False, rainbow = True):\n",
    "    \"\"\"\n",
    "    Takes in a list of images, returns a list of (x, y) keypoints for each image, in order\n",
    "    \n",
    "    Returns a list of keypoints in the shape (images, num_keypoints, 2 (since x and y) )\n",
    "    \n",
    "    Optional Args:\n",
    "    include_edges - boolean for whether or not to count the edges of the image as keypoints automatically\n",
    "    dims - the number of images, triangulations, and points to display as we decide to keep going or quit\n",
    "    timeout - the time after which no click closes out the program\n",
    "    cmap - color map option is nice if you're dealing with grayscale images\n",
    "    mesh - Whether or not to display a mesh from the keypoints\n",
    "    rainbow - whether or not to color the points with a rainbow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure all of our images are properly sized\n",
    "    for i, image in enumerate(images):\n",
    "        if len(image.shape) < 3:\n",
    "            images[i] = np.stack((image, image, image), axis = -1)\n",
    "    \n",
    "    # Initialize our list of keypoints as empty of the edges of the images\n",
    "    images_keypoints = [[] for image in images]\n",
    "    if include_edges:\n",
    "        images_keypoints = [[(0,0), (0, image.shape[0]-1), (image.shape[1]-1, 0), \n",
    "                             (image.shape[1]-1, image.shape[0]-1)]\n",
    "                            for image in images]\n",
    "    \n",
    "    while True:\n",
    "        # Display all the images that we have and the keypoints, and traingulations\n",
    "        plt.clf()\n",
    "        show_correspondences(images, images_keypoints, dims, cmap, mesh, rainbow, \n",
    "                             title = \"Click to add another round of points, or press enter\")\n",
    "            \n",
    "        # See if we want to add more keypoints\n",
    "        if plt.waitforbuttonpress():\n",
    "            plt.close()\n",
    "            return images_keypoints\n",
    "        plt.clf()\n",
    "        \n",
    "        # Start adding new points to a list\n",
    "        plt.title(\"Left click for the next point or hit enter to escape\")\n",
    "        next_points = []\n",
    "        for image, keypoints in zip(images, images_keypoints):\n",
    "            plt.clf()\n",
    "            plt.imshow(image, cmap = cmap)\n",
    "            plt.scatter([point[0] for point in keypoints], [point[1] for point in keypoints])\n",
    "            plt.draw()\n",
    "            user_input = plt.ginput(1, timeout=timeout)\n",
    "            if not len(user_input):\n",
    "                return images_keypoints\n",
    "            else:\n",
    "                next_points.append(user_input[0])  # ginput returns a list of coords... want the first one\n",
    "        plt.close()\n",
    "        \n",
    "        # If we make it through all the images, add all our keypoints to our final list\n",
    "        for next_point, keypoints in zip(next_points, images_keypoints):\n",
    "            keypoints.append(next_point)\n",
    "\n",
    "def get_and_show_correspondences(images, dims, timeout = -1, include_edges = False, cmap = None, mesh = False, rainbow = True, title = None):\n",
    "    \"\"\"\n",
    "    A wrapper for the get_correspondences that also shows images and keypoints after the fact\n",
    "    Same args\n",
    "    \"\"\"\n",
    "    images_keypoints = get_correspondences(images, dims, timeout, include_edges, cmap = cmap, mesh = mesh)\n",
    "    show_correspondences(images, images_keypoints, dims, cmap, mesh, rainbow, title)\n",
    "    return images_keypoints\n",
    "\n",
    "\n",
    "def show_correspondences(images, images_keypoints, dims, cmap = None, \n",
    "                         mesh = False, rainbow = False, title = None, \n",
    "                         figsize = (10, 5)):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    images - list of images in shape (n_images, h, w, c) or (n_images, h, w)\n",
    "    images_keypoints - a list of keypoints whose i th index is (n_images, n_keypoints, 2)\n",
    "                        corresponding to images[i]\n",
    "    dims - a tuple of dimensions for our output graphic (w, h)\n",
    "\n",
    "    Optional arg: \n",
    "    cmap - a cmap for displaying images... useful for if the images are black and white\n",
    "    mesh - Whether or not to display a mesh from the keypoints\n",
    "    rainbow - whether or not to color the points with a rainbow\n",
    "    title - a title for the plot\n",
    "    figsize - the size of the figure to show\n",
    "    \"\"\"\n",
    "    images_keypoints = np.array(images_keypoints)\n",
    "    # Draw out the first product(*dims) images, keypoints, and triangulation\n",
    "    if mesh and len(images_keypoints[0]) >= 3:\n",
    "        tri = get_avg_triangulation(images_keypoints)\n",
    "    fig, axs = plt.subplots(*dims, figsize = figsize)\n",
    "    axs = np.array(axs)\n",
    "    for ax, image, keypoints in zip(axs.flatten(), images, images_keypoints):\n",
    "        ax.imshow(image, cmap = cmap)\n",
    "        color = None\n",
    "        if rainbow:\n",
    "            color = cm.rainbow(np.linspace(0, 1, len(keypoints)))\n",
    "        ax.scatter([point[0] for point in keypoints], [point[1] for point in keypoints], color = color)\n",
    "        if mesh and len(images_keypoints[0]) >= 3:\n",
    "            ax.triplot(keypoints[:,0], keypoints[:,1], tri.simplices)\n",
    "        ax.axis(\"off\")\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500bf87-424a-4e02-977f-9dbdd8e87b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71aa960a-ba10-4d6c-aaf0-19f47261fa72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homographies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1d0a2-f73f-41c0-abda-2d17ccf9c94a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40582a93-be31-417a-83af-3833131fab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import map_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805cc5e-f4b4-402f-8568-be3c59764bb8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55989b0-61c4-4cb3-80ad-1980d76d49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeH(im1_pts, im2_pts):\n",
    "    \"\"\"\n",
    "    Computes the homography from im1 to im2\n",
    "    \n",
    "    [         ]       [         ]\n",
    "    [ im2_pts ] = H @ [ im1_pts ]\n",
    "    [ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    im1_pts = list of shape (pts, 2)\n",
    "    im2_pts = list of shape (pts, 2)\n",
    "    \n",
    "    Here we use the kind of transform I talked about on my website and on the article I linked\n",
    "    You can see where I define my large A matrix of augmented data, and my b vector of outputs\n",
    "    \"\"\"\n",
    "    b = np.array([[im2_pts[i][0]] for i in range(len(im2_pts))] + \n",
    "                  [[im2_pts[i][1]] for i in range(len(im2_pts))])\n",
    "    \n",
    "    # Get the first half the rows of A\n",
    "    # x, y, 1, 0, 0, 0, -x*x_hat, -y*x_hat\n",
    "    A_1 = [[im1_pts[i][0], im1_pts[i][1], 1, 0, 0, 0,\n",
    "            -im1_pts[i][0]*im2_pts[i][0], -im1_pts[i][1]*im2_pts[i][0]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    # Get the secong half the rows of A\n",
    "    # 0, 0, 0, x, y, 1, -x*y_hat, -y*y_hat\n",
    "    A_2 = [[0, 0, 0, im1_pts[i][0], im1_pts[i][1], 1,\n",
    "            -im1_pts[i][0]*im2_pts[i][1], -im1_pts[i][1]*im2_pts[i][1]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    \n",
    "    A = np.array(A_1 + A_2)\n",
    "        \n",
    "    H_flat = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    H_flat = H_flat.flatten()\n",
    "    # Add in h_33 which is set to 1\n",
    "    H_flat = np.concatenate((H_flat, [1,]))\n",
    "    \n",
    "    H = H_flat.reshape(3, 3)\n",
    "    \n",
    "    return H\n",
    "\n",
    "\"\"\"\n",
    "def computeH(im1_pts, im2_pts):\n",
    "    #Computes the homography from im1 to im2\n",
    "    \n",
    "    #[         ]       [         ]\n",
    "    #[ im2_pts ] = H @ [ im1_pts ]\n",
    "    #[ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    #im1_pts = list of shape (pts, 2)\n",
    "    #im2_pts = list of shape (pts, 2)\n",
    "\n",
    "    #These get im1_pts, im2_pts into the form above\n",
    "    im1_pts = np.array(im1_pts).T\n",
    "    im1_pts = np.concatenate((im1_pts, np.ones((1,im1_pts.shape[1]))))\n",
    "    \n",
    "    im2_pts = np.array(im2_pts).T\n",
    "    im2_pts = np.concatenate((im2_pts, np.ones((1,im2_pts.shape[1]))))\n",
    "\n",
    "    \n",
    "    # Need to use np.solve to solve for x in b = Ax, \n",
    "    #   so need to transpose both sides, giving\n",
    "    #   im2_pts.T = im1_pts.T @ H.T\n",
    "    #   Now, im2_pts is b and im1_pts is a\n",
    "    #   Transposing resulting H.T gives answer\n",
    "    H = np.linalg.lstsq(im1_pts.T, im2_pts.T, rcond=None)[0].T\n",
    "    \n",
    "    # Rescale to make sure that bottom right entry is 1\n",
    "    \n",
    "    H = H/H[2,2]\n",
    "    \n",
    "    return H\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def warp_image(im, H, x_offset=0, y_offset=0, outsize = None):\n",
    "    \"\"\"\n",
    "    Warps im by using H\n",
    "    \n",
    "    Values that aren't in the frame are padded with a -1 to make masks for each image easier to create\n",
    "    \n",
    "    x_offset, y_offset - the amount by which we shift the output up and down to put it in frame\n",
    "    \n",
    "    outsize - the size of the ouput (x,y) which defaults to the shape of the input image\n",
    "    \"\"\"\n",
    "    if len(im.shape) == 2:\n",
    "        im = np.stack([im, im, im], axis = -1)\n",
    "        \n",
    "        \n",
    "    if outsize:\n",
    "        x_coords, y_coords = np.meshgrid(range(outsize[0]), range(outsize[1]))\n",
    "        out = np.zeros((outsize[1], outsize[0], 3))\n",
    "\n",
    "    else:\n",
    "        y_coords, x_coords = np.meshgrid(range(im.shape[0]), range(im.shape[1]))\n",
    "        out = np.zeros(im.shape)\n",
    "    \n",
    "    x_coords = x_coords.flatten()\n",
    "    y_coords = y_coords.flatten()\n",
    "    \n",
    "    coordinate_stack = np.stack([x_coords+x_offset, y_coords+y_offset, \n",
    "                                 np.ones(y_coords.shape)], axis = 0)\n",
    "        \n",
    "    transformed_coords = np.linalg.inv(H) @ coordinate_stack\n",
    "    transformed_coords /= transformed_coords[-1,:]\n",
    "        \n",
    "    for channel in range(3):\n",
    "        out[y_coords, x_coords, channel] = map_coordinates(im[:,:,channel], \n",
    "                                                           [transformed_coords[1], transformed_coords[0]],\n",
    "                                                           mode = 'constant',\n",
    "                                                           cval=-1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def mosaic_from_left(images, keypoints, transforms):\n",
    "    \"\"\"\n",
    "    Here, I just go image by image, adding the new transform to a running transform by composing the two\n",
    "    I then augment each image by the running transform to get a set of images that have all been\n",
    "    transformed properly and just need to be added and averaged in areas of overlap. \n",
    "    \"\"\"\n",
    "    x_offsets = np.array([0,0])\n",
    "    y_offsets = np.array([0,0])\n",
    "    for i in range(1, len(keypoints)):\n",
    "        offset = np.mean(np.array(pts_12)[0] - np.array(pts_12)[1], axis = 0).flatten()\n",
    "        offset = np.round(offsets).astype(int)\n",
    "        \n",
    "        y_offsets += np.array([max(offsets[1], 0), -min(offsets[1], 0)])\n",
    "        x_offsets += np.array([max(offsets[0], 0), -min(offsets[0], 0)])\n",
    "\n",
    "    im1 = np.maximum(image1, 0)\n",
    "    im2 = np.maximum(image2, 0)\n",
    "    \n",
    "    \n",
    "    # Get our output images padded\n",
    "    images = [np.pad(image, [(max(offsets[1], 0), -min(offsets[1], 0)), \n",
    "                            (max(offsets[0], 0), -min(offsets[0], 0)),\n",
    "                            (0, 0)],\n",
    "                     mode = 'constant', constant_values = -1) for image in images]\n",
    "    \n",
    "    # Warp our images\n",
    "    total_transform = np.identity(3)\n",
    "    for i in range(1, len(images)):\n",
    "        total_transform = total_transform @ transforms[i]\n",
    "        images[i] = warp_image(images[i], total_transform)\n",
    "    \n",
    "    output_image = np.sum([np.maximum(image, 0) for image in images], axis = 0)\n",
    "    \n",
    "    # Average in areas that have the negative mask still\n",
    "    for i in range(1, len(images)):\n",
    "        output_image /= np.ones(images[0].shape) + np.logical_and(images[i-1] >= 0, images[i] >= 0)\n",
    "        \n",
    "    return output_image\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9e38a-ff2c-4e25-bbf4-7fab5e822e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e15f3ba-5a58-4256-9519-f7d93c8fea88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6232e6a-2379-4a3c-a04f-0a1cc0b1f8a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2914bb-f2f4-486c-a954-af4ab85167fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [get_image(path)/256 for path in ('im1.jpg', 'im2.jpg', 'im3.jpg')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e99df-13c7-43ca-be3b-3a1388d3ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "pts_12 = get_and_show_correspondences(images[:2], (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e349ae-c78d-4a05-b986-af16c2481162",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = computeH(pts_12[0], pts_12[1])\n",
    "show(warp_image(images[0], H, x_offset = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458421c-3830-4a1d-8f11-72e8535f22ac",
   "metadata": {},
   "source": [
    "### Image Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0abe12-acf4-41a7-8949-5c5c6df8258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "mona_lisa_keypoints = get_and_show_correspondences([get_image('mona_lisa.jpeg')/256], (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d2f21-dae7-49a2-88d2-c533d2d2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rectify_mona_lisa_keypoints = [(0, 0), (600, 0), (0, 900), (600, 900)]\n",
    "\n",
    "H = computeH(mona_lisa_keypoints[0], rectify_mona_lisa_keypoints)\n",
    "\n",
    "show(warp_image(get_image('mona_lisa.jpeg')/256, H, outsize = (600, 900)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4be121-1a2c-422c-aca9-a1765f999b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c10b2e-af51-440e-b325-f19599ed12aa",
   "metadata": {},
   "source": [
    "### Testing Merging Images (Mosaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab6357-1e4d-48b1-a58b-b27d5ceeef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = np.mean(np.array(pts_12)[0] - np.array(pts_12)[1], axis = 0)\n",
    "offsets = np.round(offsets).astype(int)\n",
    "image2 = np.pad(images[0], [(max(offsets[1], 0), -min(offsets[1], 0)), \n",
    "                            (max(offsets[0], 0), -min(offsets[0], 0)),\n",
    "                            (0, 0)],\n",
    "               mode = 'constant', constant_values = -1)\n",
    "\n",
    "image2 = warp_image(image2, H)\n",
    "\n",
    "image1 = np.pad(images[1], [(max(offsets[1], 0), -min(offsets[1], 0)), \n",
    "                            (max(offsets[0], 0), -min(offsets[0], 0)),\n",
    "                            (0, 0)],\n",
    "               mode = 'constant', constant_values = -1)\n",
    "\n",
    "im1 = np.maximum(image1, 0)\n",
    "im2 = np.maximum(image2, 0)\n",
    "\n",
    "\n",
    "show((im1 + im2) / (np.ones(im1.shape) + np.logical_and(image1 >= 0, image2 >= 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aacc24-4dd4-4daa-af13-a684a359f7fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bells and Whistles: Nyan Lisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148355e1-d8ee-4444-8095-f8ae47288d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "mona_lisa_keypoints = get_and_show_correspondences([get_image('mona_lisa.jpeg')/256], (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ffa584-d90f-4502-8564-a9a47e339376",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyan_cat_keypoints = [(0, 0), (199, 0), (0, 250), (199, 250)]\n",
    "\n",
    "H = computeH(nyan_cat_keypoints, mona_lisa_keypoints[0])\n",
    "\n",
    "nyan_cat_warp = warp_image(get_image('nyan_cat.jpg')/256, H, outsize = (1000, 533))\n",
    "\n",
    "mask = (nyan_cat_warp >=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3be5e34-5d37-4b55-bd8a-50ba7c9613dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "show(nyan_cat_warp * mask + get_image('mona_lisa.jpeg')/256 * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740180-696e-4658-8d3b-794bf1090071",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correspondences([get_image('mona_lisa.jpeg')/256], mona_lisa_keypoints, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304c784-0f9b-4628-951d-b9b47a30a099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd29ce2-c081-4e34-8ce7-ee3fe5b0d36b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cc236-696d-4e3b-a773-f72c819d96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show((np.ones(im1.shape) + np.logical_and(image1 >= 0, image2 >= 0))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e87793-57a5-4f33-9956-0d6a0a234c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2afe7-16ce-43a1-8382-d930a010af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1 = np.array(mona_lisa_keypoints[0]).T\n",
    "pts1 = np.concatenate((pts1, np.ones((1,pts1.shape[1]))))\n",
    "\n",
    "pts2 = np.array(rectify_mona_lisa_keypoints).T\n",
    "pts2 = np.concatenate((pts2, np.ones((1,pts2.shape[1]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cc261-4986-4a06-888a-a342ed60c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7043c961-d7f2-4059-aec3-43a7227c6599",
   "metadata": {},
   "source": [
    "# Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67aa5cd-e137-4e71-8e70-71777afce574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeH(im1_pts, im2_pts):\n",
    "    \"\"\"\n",
    "    Computes the homography from im1 to im2\n",
    "    \n",
    "    [         ]       [         ]\n",
    "    [ im2_pts ] = H @ [ im1_pts ]\n",
    "    [ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    im1_pts = list of shape (pts, 2)\n",
    "    im2_pts = list of shape (pts, 2)\n",
    "    \"\"\"\n",
    "    b = np.array([[im2_pts[i][0]] for i in range(len(im2_pts))] + \n",
    "                  [[im2_pts[i][1]] for i in range(len(im2_pts))])\n",
    "    \n",
    "    # Get the first half the rows of A\n",
    "    # x, y, 1, 0, 0, 0, -x*x_hat, -y*x_hat\n",
    "    A_1 = [[im1_pts[i][0], im1_pts[i][1], 1, 0, 0, 0,\n",
    "            -im1_pts[i][0]*im2_pts[i][0], -im1_pts[i][1]*im2_pts[i][0]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    # Get the secong half the rows of A\n",
    "    # 0, 0, 0, x, y, 1, -x*y_hat, -y*y_hat\n",
    "    A_2 = [[0, 0, 0, im1_pts[i][0], im1_pts[i][1], 1,\n",
    "            -im1_pts[i][0]*im2_pts[i][1], -im1_pts[i][1]*im2_pts[i][1]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    \n",
    "    A = np.array(A_1 + A_2)\n",
    "        \n",
    "    H_flat = np.linalg.lstsq(A, b)[0]\n",
    "    print(np.linalg.lstsq(A, b))\n",
    "    print(H_flat)\n",
    "    H_flat = H_flat.flatten()\n",
    "    # Add in h_33 which is set to 1\n",
    "    H_flat = np.concatenate((H_flat, [1,]))\n",
    "    \n",
    "    H = H_flat.reshape(3, 3)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a25ff2-98bc-4c64-830c-84cd6d4c47e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
