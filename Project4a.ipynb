{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18d2cd9-5258-4626-83bb-bf23337fb6df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Base Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f75232d-f814-429e-ae8f-5840ec75bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from cv2 import resize\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "import skimage.io as skio\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd2bd2-7c5d-421d-aa36-6eb6758b32f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14926414-9235-4722-baef-a74eef025c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    im = plt.imread(path)\n",
    "    im = im.astype(float)\n",
    "    return im\n",
    "\n",
    "def show(im, figsize = 10, cmap=None):\n",
    "    plt.figure(figsize=(figsize,figsize))\n",
    "    plt.imshow(im, cmap=cmap)\n",
    "    \n",
    "def resize_preserve(img, shape):\n",
    "    \"\"\"\n",
    "    Here, this is to crop without warping the input image\n",
    "    \"\"\"\n",
    "    cv2_shape = np.array([img.shape[1], img.shape[0]])\n",
    "    initial_upscale = max(shape[0]/cv2_shape[0], shape[1]/cv2_shape[1])\n",
    "    img = resize(img, (cv2_shape * initial_upscale).astype(int))\n",
    "    cv2_shape = np.array([img.shape[1], img.shape[0]])\n",
    "\n",
    "    # Crop in x direction if needed\n",
    "    xdiff = img.shape[1] - shape[0]\n",
    "    if xdiff:\n",
    "        img = img[:,xdiff//2:img.shape[1]-xdiff//2]\n",
    "    \n",
    "    # Crop in y direction if needed\n",
    "    ydiff = img.shape[0] - shape[1]\n",
    "    if ydiff:\n",
    "        img = img[ydiff//2:img.shape[0]-ydiff//2]\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da35001-d31c-4669-a702-0a85e6681c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1d888-9f29-4961-8078-b4db98ff0af4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb80ca2a-11ce-4236-809a-6ae2e08c0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f8f64-d152-48f2-a469-05420aca2987",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b75544e-9be4-4610-af92-e2efaa24ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondences(images, dims = (1, 1), timeout = -1, include_edges = False, cmap = None, mesh = False, rainbow = True):\n",
    "    \"\"\"\n",
    "    Takes in a list of images, returns a list of (x, y) keypoints for each image, in order\n",
    "    \n",
    "    Returns a list of keypoints in the shape (images, num_keypoints, 2 (since x and y) )\n",
    "    \n",
    "    Optional Args:\n",
    "    include_edges - boolean for whether or not to count the edges of the image as keypoints automatically\n",
    "    dims - the number of images, triangulations, and points to display as we decide to keep going or quit\n",
    "    timeout - the time after which no click closes out the program\n",
    "    cmap - color map option is nice if you're dealing with grayscale images\n",
    "    mesh - Whether or not to display a mesh from the keypoints\n",
    "    rainbow - whether or not to color the points with a rainbow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure all of our images are properly sized\n",
    "    for i, image in enumerate(images):\n",
    "        if len(image.shape) < 3:\n",
    "            images[i] = np.stack((image, image, image), axis = -1)\n",
    "    \n",
    "    # Initialize our list of keypoints as empty of the edges of the images\n",
    "    images_keypoints = [[] for image in images]\n",
    "    if include_edges:\n",
    "        images_keypoints = [[(0,0), (0, image.shape[0]-1), (image.shape[1]-1, 0), \n",
    "                             (image.shape[1]-1, image.shape[0]-1)]\n",
    "                            for image in images]\n",
    "    \n",
    "    while True:\n",
    "        # Display all the images that we have and the keypoints, and traingulations\n",
    "        plt.clf()\n",
    "        show_correspondences(images, images_keypoints, dims, cmap, mesh, rainbow, \n",
    "                             title = \"Click to add another round of points, or press enter\")\n",
    "            \n",
    "        # See if we want to add more keypoints\n",
    "        if plt.waitforbuttonpress():\n",
    "            plt.close()\n",
    "            return images_keypoints\n",
    "        plt.clf()\n",
    "        \n",
    "        # Start adding new points to a list\n",
    "        plt.title(\"Left click for the next point or hit enter to escape\")\n",
    "        next_points = []\n",
    "        for image, keypoints in zip(images, images_keypoints):\n",
    "            plt.clf()\n",
    "            plt.imshow(image, cmap = cmap)\n",
    "            plt.scatter([point[0] for point in keypoints], [point[1] for point in keypoints])\n",
    "            plt.draw()\n",
    "            user_input = plt.ginput(1, timeout=timeout)\n",
    "            if not len(user_input):\n",
    "                return images_keypoints\n",
    "            else:\n",
    "                next_points.append(user_input[0])  # ginput returns a list of coords... want the first one\n",
    "        plt.close()\n",
    "        \n",
    "        # If we make it through all the images, add all our keypoints to our final list\n",
    "        for next_point, keypoints in zip(next_points, images_keypoints):\n",
    "            keypoints.append(next_point)\n",
    "\n",
    "def get_and_show_correspondences(images, dims, timeout = -1, include_edges = False, cmap = None, mesh = False, rainbow = True, title = None):\n",
    "    \"\"\"\n",
    "    A wrapper for the get_correspondences that also shows images and keypoints after the fact\n",
    "    Same args\n",
    "    \"\"\"\n",
    "    images_keypoints = get_correspondences(images, dims, timeout, include_edges, cmap = cmap, mesh = mesh)\n",
    "    show_correspondences(images, images_keypoints, dims, cmap, mesh, rainbow, title)\n",
    "    return images_keypoints\n",
    "\n",
    "\n",
    "def show_correspondences(images, images_keypoints, dims, cmap = None, \n",
    "                         mesh = False, rainbow = False, title = None, \n",
    "                         figsize = (10, 5)):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    images - list of images in shape (n_images, h, w, c) or (n_images, h, w)\n",
    "    images_keypoints - a list of keypoints whose i th index is (n_images, n_keypoints, 2)\n",
    "                        corresponding to images[i]\n",
    "    dims - a tuple of dimensions for our output graphic (w, h)\n",
    "\n",
    "    Optional arg: \n",
    "    cmap - a cmap for displaying images... useful for if the images are black and white\n",
    "    mesh - Whether or not to display a mesh from the keypoints\n",
    "    rainbow - whether or not to color the points with a rainbow\n",
    "    title - a title for the plot\n",
    "    figsize - the size of the figure to show\n",
    "    \"\"\"\n",
    "    images_keypoints = np.array(images_keypoints)\n",
    "    # Draw out the first product(*dims) images, keypoints, and triangulation\n",
    "    if mesh and len(images_keypoints[0]) >= 3:\n",
    "        tri = get_avg_triangulation(images_keypoints)\n",
    "    fig, axs = plt.subplots(*dims, figsize = figsize)\n",
    "    axs = np.array(axs)\n",
    "    for ax, image, keypoints in zip(axs.flatten(), images, images_keypoints):\n",
    "        ax.imshow(image, cmap = cmap)\n",
    "        color = None\n",
    "        if rainbow:\n",
    "            color = cm.rainbow(np.linspace(0, 1, len(keypoints)))\n",
    "        ax.scatter([point[0] for point in keypoints], [point[1] for point in keypoints], color = color)\n",
    "        if mesh and len(images_keypoints[0]) >= 3:\n",
    "            ax.triplot(keypoints[:,0], keypoints[:,1], tri.simplices)\n",
    "        ax.axis(\"off\")\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500bf87-424a-4e02-977f-9dbdd8e87b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71aa960a-ba10-4d6c-aaf0-19f47261fa72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homographies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1d0a2-f73f-41c0-abda-2d17ccf9c94a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40582a93-be31-417a-83af-3833131fab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import map_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805cc5e-f4b4-402f-8568-be3c59764bb8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55989b0-61c4-4cb3-80ad-1980d76d49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeH(im1_pts, im2_pts):\n",
    "    \"\"\"\n",
    "    Computes the homography from im1 to im2\n",
    "    \n",
    "    [         ]       [         ]\n",
    "    [ im2_pts ] = H @ [ im1_pts ]\n",
    "    [ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    im1_pts = list of shape (pts, 2)\n",
    "    im2_pts = list of shape (pts, 2)\n",
    "    \n",
    "    Here we use the kind of transform I talked about on my website and on the article I linked\n",
    "    You can see where I define my large A matrix of augmented data, and my b vector of outputs\n",
    "    \"\"\"\n",
    "    b = np.array([[im2_pts[i][0]] for i in range(len(im2_pts))] + \n",
    "                  [[im2_pts[i][1]] for i in range(len(im2_pts))])\n",
    "    \n",
    "    # Get the first half the rows of A\n",
    "    # x, y, 1, 0, 0, 0, -x*x_hat, -y*x_hat\n",
    "    A_1 = [[im1_pts[i][0], im1_pts[i][1], 1, 0, 0, 0,\n",
    "            -im1_pts[i][0]*im2_pts[i][0], -im1_pts[i][1]*im2_pts[i][0]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    # Get the secong half the rows of A\n",
    "    # 0, 0, 0, x, y, 1, -x*y_hat, -y*y_hat\n",
    "    A_2 = [[0, 0, 0, im1_pts[i][0], im1_pts[i][1], 1,\n",
    "            -im1_pts[i][0]*im2_pts[i][1], -im1_pts[i][1]*im2_pts[i][1]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    \n",
    "    A = np.array(A_1 + A_2)\n",
    "        \n",
    "    H_flat = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    H_flat = H_flat.flatten()\n",
    "    # Add in h_33 which is set to 1\n",
    "    H_flat = np.concatenate((H_flat, [1,]))\n",
    "    \n",
    "    H = H_flat.reshape(3, 3)\n",
    "    \n",
    "    return H\n",
    "\n",
    "\"\"\"\n",
    "def computeH(im1_pts, im2_pts):\n",
    "    #Computes the homography from im1 to im2\n",
    "    \n",
    "    #[         ]       [         ]\n",
    "    #[ im2_pts ] = H @ [ im1_pts ]\n",
    "    #[ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    #im1_pts = list of shape (pts, 2)\n",
    "    #im2_pts = list of shape (pts, 2)\n",
    "\n",
    "    #These get im1_pts, im2_pts into the form above\n",
    "    im1_pts = np.array(im1_pts).T\n",
    "    im1_pts = np.concatenate((im1_pts, np.ones((1,im1_pts.shape[1]))))\n",
    "    \n",
    "    im2_pts = np.array(im2_pts).T\n",
    "    im2_pts = np.concatenate((im2_pts, np.ones((1,im2_pts.shape[1]))))\n",
    "\n",
    "    \n",
    "    # Need to use np.solve to solve for x in b = Ax, \n",
    "    #   so need to transpose both sides, giving\n",
    "    #   im2_pts.T = im1_pts.T @ H.T\n",
    "    #   Now, im2_pts is b and im1_pts is a\n",
    "    #   Transposing resulting H.T gives answer\n",
    "    H = np.linalg.lstsq(im1_pts.T, im2_pts.T, rcond=None)[0].T\n",
    "    \n",
    "    # Rescale to make sure that bottom right entry is 1\n",
    "    \n",
    "    H = H/H[2,2]\n",
    "    \n",
    "    return H\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def warp_image(im, H, x_offset=0, y_offset=0, outsize = None):\n",
    "    \"\"\"\n",
    "    Warps im by using H\n",
    "    \n",
    "    Values that aren't in the frame are padded with a -1 to make masks for each image easier to create\n",
    "    \n",
    "    x_offset, y_offset - the amount by which we shift the output up and down to put it in frame\n",
    "    \n",
    "    outsize - the size of the ouput (x,y) which defaults to the shape of the input image\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(im.shape) == 2:\n",
    "        im = np.stack([im, im, im], axis = -1)\n",
    "        \n",
    "        \n",
    "    if outsize:\n",
    "        x_coords, y_coords = np.meshgrid(range(outsize[0]), range(outsize[1]))\n",
    "        out = np.zeros((outsize[1], outsize[0], 3))\n",
    "\n",
    "    else:\n",
    "        y_coords, x_coords = np.meshgrid(range(im.shape[0]), range(im.shape[1]))\n",
    "        out = np.zeros(im.shape)\n",
    "    \n",
    "    x_coords = x_coords.flatten()\n",
    "    y_coords = y_coords.flatten()\n",
    "    \n",
    "    coordinate_stack = np.stack([x_coords+x_offset, y_coords+y_offset, \n",
    "                                 np.ones(y_coords.shape)], axis = 0)\n",
    "        \n",
    "    transformed_coords = np.linalg.inv(H) @ coordinate_stack\n",
    "    transformed_coords /= transformed_coords[-1,:]\n",
    "        \n",
    "    for channel in range(3):\n",
    "        out[y_coords, x_coords, channel] = map_coordinates(im[:,:,channel], \n",
    "                                                           [transformed_coords[1], transformed_coords[0]],\n",
    "                                                           mode = 'constant',\n",
    "                                                           cval=-1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def fast_warp_image(im, H, x_offset=0, y_offset=0):\n",
    "    im = np.swapaxes(im, 0, 1)\n",
    "    \n",
    "    if len(im.shape) == 2:\n",
    "        im = np.stack([im, im, im], axis = -1)\n",
    "        \n",
    "    y_coords, x_coords = np.meshgrid(range(im.shape[1]), range(im.shape[0]))\n",
    "    \n",
    "    map_shape = y_coords.shape\n",
    "    \n",
    "    x_coords = x_coords.flatten()\n",
    "    y_coords = y_coords.flatten()\n",
    "    \n",
    "    coordinate_stack = np.stack([x_coords+x_offset, y_coords+y_offset, \n",
    "                                 np.ones(y_coords.shape)], axis = 0)\n",
    "        \n",
    "    transformed_coords = np.linalg.inv(H) @ coordinate_stack\n",
    "\n",
    "    transformed_coords /= transformed_coords[-1,:]\n",
    "        \n",
    "    transformed_coords = transformed_coords[:-1].reshape(2, map_shape[0], map_shape[1]).astype(np.float32)\n",
    "\n",
    "    out = cv2.remap(im, transformed_coords[1].T, transformed_coords[0].T, cv2.INTER_LINEAR, borderValue = -1)\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def mosaic_from_left(images, keypoint_pairs = None, padding = [(0, 0), (0, 0)], middle_idx = None, correspondence_fn = get_correspondences):\n",
    "    \"\"\"\n",
    "    Here, we take the center image in images, and gather a mosaic, returning the mosaic,\n",
    "    the input images, the keypoints, and the transforms to get from one to the next\n",
    "    \n",
    "    Images are assumed to be from right to left\n",
    "    \"\"\"\n",
    "    \n",
    "    if not middle_idx:\n",
    "        middle_idx = len(images)//2\n",
    "    \n",
    "    if not keypoint_pairs:\n",
    "        keypoint_pairs = []\n",
    "\n",
    "        for i in range(len(images) - 1):\n",
    "            keypoint_pairs.append(correspondence_fn(images[i:i+2]))\n",
    "            \n",
    "    \n",
    "    # This should be a list of arrays... just want to make sure we cast each pair of keypoints right\n",
    "    # list of size (no_pairs, 2 (one for each image in pair), x, y)\n",
    "    keypoint_pairs = [np.array(keypoint_pairs[i]) for i in range(len(keypoint_pairs))]\n",
    "    \n",
    "    \n",
    "    images = [np.pad(image, [padding[1], padding[0], (0, 0)], \n",
    "                     mode = 'constant', constant_values = -1)\n",
    "             for image in images]\n",
    "    \n",
    "    \n",
    "    # Make sure our keypoints have their x, y values updated with low side of x, y pads\n",
    "    for keypoint_pair in keypoint_pairs:\n",
    "        keypoint_pair[:,:,0] += padding[0][0]\n",
    "        keypoint_pair[:,:,1] += padding[1][0]\n",
    "    \n",
    "    \n",
    "    # Do the homographies from the left of center. This is from the pair before the middle pair to the\n",
    "    #    leftmost image\n",
    "    #    We are going FROM the center TO the start, \n",
    "    #    so we need go go from keypoint_pair[1] -> keypoint_pair[0]\n",
    "    transforms = np.identity(3)\n",
    "    for i in reversed(range(middle_idx)):\n",
    "        H = computeH(keypoint_pairs[i][0], keypoint_pairs[i][1])\n",
    "        transforms = H @ transforms\n",
    "        images[i] = fast_warp_image(images[i], transforms)\n",
    "        \n",
    "        \n",
    "    # Do the homographies from the right of center. This is from the middle pair to the\n",
    "    #    rightmost image\n",
    "    #    The image we are replacing will be the i+1th image, as it is the second image in the\n",
    "    #    pair\n",
    "    #    We are going FROM the center TO the end, \n",
    "    #    so we need go go from keypoint_pair[0] -> keypoint_pair[1]\n",
    "    transforms = np.identity(3)\n",
    "    for i in range(middle_idx, len(keypoint_pairs)):\n",
    "        H = computeH(keypoint_pairs[i][1], keypoint_pairs[i][0])\n",
    "        transforms = H @ transforms\n",
    "        images[i+1] = fast_warp_image(images[i+1], transforms)\n",
    "    \n",
    "    \n",
    "    mask = np.sum([images[i][:,:,0] >= 0 for i in range(len(images))], axis = 0)\n",
    "    mask = np.clip(mask, 1, float('inf'))\n",
    "    mask = np.stack([mask, mask, mask], axis = -1)\n",
    "    \n",
    "    output = np.sum([np.clip(image, 0, 1) for image in images], axis = 0)\n",
    "    output = output/mask\n",
    "    \n",
    "    return output, images, keypoint_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9e38a-ff2c-4e25-bbf4-7fab5e822e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e15f3ba-5a58-4256-9519-f7d93c8fea88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6232e6a-2379-4a3c-a04f-0a1cc0b1f8a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db2914bb-f2f4-486c-a954-af4ab85167fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic1_images = [get_image(path)/256 for path in ('im1_1.jpg', 'im2_1.jpg', 'im3_1.jpg')] \n",
    "mosaic2_images = [get_image(path)/256 for path in ('im1_2.jpg', 'im2_2.jpg', 'im3_2.jpg')] \n",
    "mosaic3_images = [get_image(path)/256 for path in ('im1_3.jpg', 'im2_3.jpg', 'im3_3.jpg', 'im4_3.jpg', 'im5_3.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e99df-13c7-43ca-be3b-3a1388d3ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "pts_12 = get_and_show_correspondences(images[:2], (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e349ae-c78d-4a05-b986-af16c2481162",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = computeH(_keypoint_pairs[1][1], _keypoint_pairs[1][0])\n",
    "show(warp_image(images[2], H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847daa9-5982-4612-a768-a2eeb9e3ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458421c-3830-4a1d-8f11-72e8535f22ac",
   "metadata": {},
   "source": [
    "### Image Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d0abe12-acf4-41a7-8949-5c5c6df8258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "mona_lisa_keypoints = get_and_show_correspondences([get_image('im1_1.jpg')/256], (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "192d2f21-dae7-49a2-88d2-c533d2d2fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "rectify_mona_lisa_keypoints = [(0, 0), (1000, 0), (0, 1200), (1000, 1200)]\n",
    "\n",
    "H = computeH(mona_lisa_keypoints[0], rectify_mona_lisa_keypoints)\n",
    "\n",
    "show(warp_image(get_image('im1_1.jpg')/256, H, outsize = (1000, 1200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4be121-1a2c-422c-aca9-a1765f999b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c10b2e-af51-440e-b325-f19599ed12aa",
   "metadata": {},
   "source": [
    "### Testing Merging Images (Mosaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c8b2241-18b8-497b-853d-a87ddd64a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(mosaic_from_left(mosaic3_images[:2], keypoint_pairs = None, padding = [(0, 2000), (2000, 2000)], middle_idx = None, correspondence_fn = get_correspondences)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aacc24-4dd4-4daa-af13-a684a359f7fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bells and Whistles: Nyan Lisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148355e1-d8ee-4444-8095-f8ae47288d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "mona_lisa_keypoints = get_and_show_correspondences([get_image('mona_lisa.jpeg')/256], (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffa584-d90f-4502-8564-a9a47e339376",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyan_cat_keypoints = [(0, 0), (199, 0), (0, 250), (199, 250)]\n",
    "\n",
    "H = computeH(nyan_cat_keypoints, mona_lisa_keypoints[0])\n",
    "\n",
    "nyan_cat_warp = warp_image(get_image('nyan_cat.jpg')/256, H, outsize = (1000, 533))\n",
    "\n",
    "mask = (nyan_cat_warp >=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be5e34-5d37-4b55-bd8a-50ba7c9613dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(nyan_cat_warp * mask + get_image('mona_lisa.jpeg')/256 * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740180-696e-4658-8d3b-794bf1090071",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correspondences([get_image('mona_lisa.jpeg')/256], mona_lisa_keypoints, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304c784-0f9b-4628-951d-b9b47a30a099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd29ce2-c081-4e34-8ce7-ee3fe5b0d36b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cc236-696d-4e3b-a773-f72c819d96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show((np.ones(im1.shape) + np.logical_and(image1 >= 0, image2 >= 0))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e87793-57a5-4f33-9956-0d6a0a234c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2afe7-16ce-43a1-8382-d930a010af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1 = np.array(mona_lisa_keypoints[0]).T\n",
    "pts1 = np.concatenate((pts1, np.ones((1,pts1.shape[1]))))\n",
    "\n",
    "pts2 = np.array(rectify_mona_lisa_keypoints).T\n",
    "pts2 = np.concatenate((pts2, np.ones((1,pts2.shape[1]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cc261-4986-4a06-888a-a342ed60c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bd305c0-1ee4-4bb7-81ad-835ceab2fc7d",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3b589-0f0f-4744-82dc-219d337562a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bebae5c2-07f2-4209-bd43-37e7390b67a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:50: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:50: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/1691399795.py:50: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(dimx == dimc, 'Data dimension does not match dimension of centers')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import corner_harris, peak_local_max\n",
    "\n",
    "\n",
    "def get_harris_corners(im, edge_discard=20):\n",
    "    \"\"\"\n",
    "    This function takes a b&w image and an optional amount to discard\n",
    "    on the edge (default is 5 pixels), and finds all harris corners\n",
    "    in the image. Harris corners near the edge are discarded and the\n",
    "    coordinates of the remaining corners are returned. A 2d array (h)\n",
    "    containing the h value of every pixel is also returned.\n",
    "\n",
    "    h is the same shape as the original image, im.\n",
    "    coords is 2 x n (ys, xs).\n",
    "    \"\"\"\n",
    "\n",
    "    assert edge_discard >= 20\n",
    "\n",
    "    # find harris corners\n",
    "    h = corner_harris(im, method='eps', sigma=1)\n",
    "    coords = peak_local_max(h, min_distance=1, indices=True)\n",
    "\n",
    "    # discard points on edge\n",
    "    edge = edge_discard  # pixels\n",
    "    mask = (coords[:, 0] > edge) & \\\n",
    "           (coords[:, 0] < im.shape[0] - edge) & \\\n",
    "           (coords[:, 1] > edge) & \\\n",
    "           (coords[:, 1] < im.shape[1] - edge)\n",
    "    coords = coords[mask].T\n",
    "    return h, coords\n",
    "\n",
    "\n",
    "def dist2(x, c):\n",
    "    \"\"\"\n",
    "    dist2  Calculates squared distance between two sets of points.\n",
    "\n",
    "    Description\n",
    "    D = DIST2(X, C) takes two matrices of vectors and calculates the\n",
    "    squared Euclidean distance between them.  Both matrices must be of\n",
    "    the same column dimension.  If X has M rows and N columns, and C has\n",
    "    L rows and N columns, then the result has M rows and L columns.  The\n",
    "    I, Jth entry is the  squared distance from the Ith row of X to the\n",
    "    Jth row of C.\n",
    "\n",
    "    Adapted from code by Christopher M Bishop and Ian T Nabney.\n",
    "    \"\"\"\n",
    "    \n",
    "    ndata, dimx = x.shape\n",
    "    ncenters, dimc = c.shape\n",
    "    assert(dimx == dimc, 'Data dimension does not match dimension of centers')\n",
    "\n",
    "    return (np.ones((ncenters, 1)) * np.sum((x**2).T, axis=0)).T + \\\n",
    "            np.ones((   ndata, 1)) * np.sum((c**2).T, axis=0)    - \\\n",
    "            2 * np.inner(x, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb0e9f-ce80-43f6-923f-194b357271ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "330789dc-6335-4051-acf6-a15a415ff197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANMS(image, n_ip = 500, harris_threshold_scaled = .1, c_robust = .9, h = None, edge_fraction = .02):\n",
    "    \"\"\"\n",
    "    This should return a list of coordinates for a given image\n",
    "    \n",
    "    Return: list of x,y coordinates, list of radius for each coordinate\n",
    "    \"\"\"\n",
    "    # Make sure the image has 3 color channels... even if grayscale\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image, image, image])\n",
    "    \n",
    "    if h is None:\n",
    "        h, coords = get_harris_corners(np.mean(image, axis = 2))\n",
    "        mask = np.zeros(h.shape)\n",
    "        #Coords is (y, x)\n",
    "        assert mask.shape == h.shape\n",
    "        mask[coords[0],coords[1]] = 1\n",
    "        h *= mask\n",
    "    \n",
    "    \n",
    "    # discard points on edge\n",
    "    h[-int(h.shape[0]*edge_fraction):,:] = 0\n",
    "    h[:int(h.shape[0]*edge_fraction),:] = 0\n",
    "    h[:,-int(h.shape[1]*edge_fraction):] = 0\n",
    "    h[:,:int(h.shape[1]*edge_fraction)] = 0\n",
    "    \n",
    "    # Apply the thresholding to the harris detector to decrease number of points\n",
    "    h = h * (h > (np.max(h) * harris_threshold_scaled))\n",
    "    \n",
    "    # Gets the dimensions of coordinates along 1st and second dimensions\n",
    "    coords1, coords2 = np.where(h)\n",
    "    \n",
    "    r_vals = []\n",
    "    \n",
    "    for c1, c2 in tqdm(zip(coords1, coords2), total = len(coords1)):\n",
    "        r = 0\n",
    "        reference_h = h[c1,c2]\n",
    "        while True:\n",
    "            r += 1\n",
    "            lower1 = max(c1 - r, 0)\n",
    "            upper1 = min(c1 + r + 1, image.shape[0] - 1)\n",
    "            lower2 = max(c2 - r, 0)\n",
    "            upper2 = min(c2 + r + 1, image.shape[1] - 1)\n",
    "            \n",
    "            if True in (reference_h < c_robust * h[lower1, lower2:upper2]) \\\n",
    "                    or True in (reference_h < c_robust * h[upper1, lower2:upper2]) \\\n",
    "                    or True in (reference_h < c_robust * h[lower1:upper1, lower2]) \\\n",
    "                    or True in (reference_h < c_robust * h[lower1:upper1, upper2]):\n",
    "                break\n",
    "            \n",
    "            if r > max(image.shape):\n",
    "                break\n",
    "            \n",
    "        r_vals.append([(c1,c2),r])\n",
    "    \n",
    "    r_vals.sort(key = lambda x: x[-1])\n",
    "    \n",
    "    return np.array([(r_vals[-i][0][1], r_vals[-i][0][0]) for i in range(1, min(n_ip, len(r_vals)) + 1)]), \\\n",
    "            np.array([r_vals[-i][1] for i in range(1, min(n_ip, len(r_vals)) + 1)])\n",
    "\n",
    "\n",
    "def feature_descriptor(image, coords, stride = 5, w_size = 8):\n",
    "    if len(image.shape) == 3:\n",
    "        image = np.mean(image, axis = 2)\n",
    "    \n",
    "    # Since we are sampling in the middle of each of the w_size x w_size patches of size stride... \n",
    "    #.    that is w_size - 1 jumps of size stride... and we want half width\n",
    "    sampling_half_width = (w_size-1) * stride / 2\n",
    "    \n",
    "    xs = np.concatenate([np.meshgrid([coord[0] - sampling_half_width + stride * i for i in range(w_size)],\n",
    "                                     [coord[1] - sampling_half_width + stride * i for i in range(w_size)])[0]\n",
    "                         for coord in coords], axis = -1)\n",
    "    ys = np.concatenate([np.meshgrid([coord[0] - sampling_half_width + stride * i for i in range(w_size)],\n",
    "                                     [coord[1] - sampling_half_width + stride * i for i in range(w_size)])[1]\n",
    "                         for coord in coords], axis = -1)\n",
    "    \n",
    "    #assert xs.shape[0] == xs.shape[1] and xs.shape[0] == w_size\n",
    "    \n",
    "    out = cv2.remap(image.astype(np.float32), xs.astype(np.float32), ys.astype(np.float32), cv2.INTER_LINEAR)\n",
    "    out = np.split(out, len(coords), axis = -1)\n",
    "    out = out - np.expand_dims(np.min(out, axis = (-1, -2)), [-1, -2])\n",
    "    out = out / np.expand_dims(np.max(out, axis = (-1, -2)), [-1, -2])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def feature_matcher(features1, features2):\n",
    "    \"\"\"\n",
    "    Returns the scores and coords respectivley for matches in features2 to each features1[i]\n",
    "    \"\"\"\n",
    "    ssd = dist2(features1, features2)\n",
    "    ssd_partition = np.partition(ssd, [0, 1], axis = 1)\n",
    "    arg_partition = np.argpartition(ssd, [0, 1], axis = 1)\n",
    "    return ssd_partition[:,0]/ssd_partition[:,1], arg_partition[:,:2], ssd_partition[:,0]\n",
    "    \n",
    "    \n",
    "def ransac(image1, image2, poi_coords = None, iterations = 1000, feature_matching_threshold = .25, \n",
    "           inlier_threshold = 2, w_size = 8, stride = 5, edge_fraction = .02, n_ip = 500, min_features = 8, debug = False):\n",
    "\n",
    "    if not poi_coords:\n",
    "        # Generate our interest points. Shape (n_ip, 2)\n",
    "        poi_coords1, _ = ANMS(image1, n_ip = n_ip, edge_fraction = edge_fraction)\n",
    "        poi_coords2, _ = ANMS(image2, n_ip = n_ip, edge_fraction = edge_fraction)\n",
    "    else:\n",
    "        poi_coords1 = np.array(poi_coords[0])\n",
    "        poi_coords2 = np.array(poi_coords[1])\n",
    "\n",
    "    \n",
    "    # Generate the features themselves\n",
    "    features1 = feature_descriptor(image1, poi_coords1, w_size = w_size, stride = stride)\n",
    "    features2 = feature_descriptor(image2, poi_coords2, w_size = w_size, stride = stride)\n",
    "\n",
    "    # Match the features. Matches[i] should be j for features2[j] matched to features1[i]\n",
    "    if debug:\n",
    "        print(features1.shape, features2.shape)\n",
    "    scores, two_best_matches, best_score = feature_matcher(features1.reshape(poi_coords1.shape[0], -1), features2.reshape(poi_coords1.shape[0], -1))\n",
    "    matches = np.array([[i, two_best_matches[i][0]] for i in range(len(scores))])\n",
    "    if debug:\n",
    "        print(scores.shape, matches.shape)\n",
    "\n",
    "    matches = matches[np.where(scores < feature_matching_threshold)]\n",
    "    matched_scores = scores[np.where(scores < feature_matching_threshold)]\n",
    "    \n",
    "    \n",
    "    # This is the case where there aren't enough features to match... return identity H and empty keypoints, inliers, and matches\n",
    "    if len(set(match[1] for match in matches)) < 4:\n",
    "        return np.identity(3), np.zeros((2,0,2)), np.zeros((0,2)), np.zeros((0,2))\n",
    "    \n",
    "    if debug:\n",
    "        print(matches)\n",
    "        print(poi_coords2[matches[:,1]])\n",
    " \n",
    "    \n",
    "    \n",
    "    # Iterate a set number of times\n",
    "    inliers = []\n",
    "    inliers_max = 0\n",
    "    while inliers_max < min_features:\n",
    "        if inliers_max > 0:\n",
    "            print(\"Running another iteration. Our maximum set of inliers is smaller than the argument min_features\")\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            # Choose 4 matches. Pairings[0 1 2 3] is [i,j] for matched features1[i] to features2[j]\n",
    "            # We also need to make sure that no two points map to the same point, since this would be a bad transformation\n",
    "            pick_again = True\n",
    "            while pick_again:\n",
    "                pair_choices = np.random.choice(len(matches), 4)\n",
    "                pairings = matches[pair_choices]\n",
    "                if len(set([pairings[i,1] for i in range(4)])) == 4:\n",
    "                    pick_again = False\n",
    "\n",
    "            if iteration == 0 and debug:\n",
    "                print(pair_choices.shape)\n",
    "                print(pairings.shape)\n",
    "\n",
    "            # Gets the starting coordinates for features1 (starting) and features2 (ending)\n",
    "            starting_coord_idx = pairings[:,0]\n",
    "            starting_coords = poi_coords1[starting_coord_idx]\n",
    "            ending_coord_idx = pairings[:,1]\n",
    "            ending_coords = poi_coords2[ending_coord_idx]\n",
    "            if iteration == 0 and debug:\n",
    "                print(starting_coord_idx.shape, starting_coord_idx)\n",
    "                print(starting_coords.shape, starting_coords)\n",
    "                print(ending_coord_idx.shape, ending_coord_idx)\n",
    "                print(ending_coords.shape, ending_coords)\n",
    "\n",
    "            H = computeH(starting_coords, ending_coords)\n",
    "            #if iteration == 0:\n",
    "            #    show(warp_image(images[0], H))\n",
    "\n",
    "            # Order all the original point of interest coords by their idx in matches[:,0]\n",
    "            all_starting_coords = np.stack([poi_coords1[:,0][matches[:,0]], poi_coords1[:,1][matches[:,0]], \n",
    "                                            np.ones(poi_coords1[:,0][matches[:,0]].shape)], axis = 0)\n",
    "            # Order all the original point of interest coords by their idx in matches[:,1]\n",
    "            #                                       xs                                 ys                  1s\n",
    "            all_ending_coords = np.stack([poi_coords2[:,0][matches[:,1]], poi_coords2[:,1][matches[:,1]], \n",
    "                                          np.ones(poi_coords2[:,0][matches[:,1]].shape)], axis = 0)\n",
    "\n",
    "            if iteration == 0 and debug:\n",
    "                print(all_starting_coords.shape, all_ending_coords.shape)\n",
    "                print(all_starting_coords[:,:5], all_ending_coords[:,:5])\n",
    "            transformed_starting = H @ all_starting_coords\n",
    "            transformed_starting /= transformed_starting[-1]\n",
    "\n",
    "            distances = np.linalg.norm(all_ending_coords - transformed_starting, axis = 0)\n",
    "            if iteration == 0 and debug:\n",
    "                print(distances.shape, distances)\n",
    "\n",
    "            assert distances.shape[0] == matches.shape[0]\n",
    "\n",
    "\n",
    "            # We need to make sure that inliers_temp doesn't just map everything to a single coordinate... \n",
    "            #.   make sure that the size of our inliers is the number of distinct output mappings\n",
    "            inliers_temp = matches[np.where(distances < inlier_threshold)]\n",
    "            unique_mappings = set([inlier[1] for inlier in inliers_temp])\n",
    "            if len(unique_mappings) > inliers_max:\n",
    "                inliers = matches[np.where(distances < inlier_threshold)]\n",
    "                inliers_max = len(unique_mappings)\n",
    "    \n",
    "    if debug:\n",
    "        print(inliers.shape)\n",
    "    starting_coord_idx = inliers[:,0]\n",
    "    starting_coords = poi_coords1[starting_coord_idx]\n",
    "    ending_coord_idx = inliers[:,1]\n",
    "    ending_coords = poi_coords2[ending_coord_idx]\n",
    "    H_final = computeH(starting_coords, ending_coords)\n",
    "    \n",
    "    final_keypoints = np.array([starting_coords, ending_coords])\n",
    "        \n",
    "    return H_final, final_keypoints, inliers, matches #, starting_coords, ending_coords, all_starting_coords, all_ending_coords\n",
    "\n",
    "\n",
    "def auto_mosaic_ordered(images, padding = [(0, 0), (0, 0)], middle_idx = None, interest_points = None,\n",
    "               feature_matching_threshold = .8, iterations = 50000, inlier_threshold=1, w_size = 8, stride = 5):\n",
    "\n",
    "    if not interest_points:\n",
    "        print(\"Getting interest points\")\n",
    "        interest_points = []\n",
    "        for image in images:\n",
    "            interest_points.append(ANMS(image, n_ip = 2000, harris_threshold_scaled = .2, edge_fraction = .02)[0])\n",
    "\n",
    "    print(\"RANSACing\")\n",
    "    keypoint_pairs = []\n",
    "    for i in range(len(images) - 1):\n",
    "        _, final_keypoints, _, _ = ransac(images[i], images[i+1], poi_coords = (interest_points[i], interest_points[i+1]), feature_matching_threshold = feature_matching_threshold, \n",
    "                                          iterations = iterations, inlier_threshold = inlier_threshold, w_size = w_size, stride = stride)\n",
    "        keypoint_pairs.append(final_keypoints)\n",
    "\n",
    "    print(\"Stitching\")\n",
    "    output, images, keypoint_pairs = mosaic_from_left(images, keypoint_pairs = keypoint_pairs, padding = padding)\n",
    "    \n",
    "    return output, images, keypoint_pairs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ccde7-21fa-4f9d-be51-ac5a1780900b",
   "metadata": {},
   "source": [
    "Harris Edges Demo (NOTE: Images loaded from previous testing section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25ccedcd-fc3a-4b54-b79b-57c80b31ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/1691399795.py:21: FutureWarning: indices argument is deprecated and will be removed in version 0.20. To avoid this warning, please do not use the indices argument. Please see peak_local_max documentation for more details.\n",
      "  coords = peak_local_max(h, min_distance=1, indices=True)\n"
     ]
    }
   ],
   "source": [
    "h, coords = get_harris_corners(np.mean(mosaic1_images[0], axis = -1), edge_discard=20)\n",
    "h *= h > (np.max(h) * .1)\n",
    "show(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe826c-1b7c-4dfa-968a-be8988c2739b",
   "metadata": {},
   "source": [
    "ANMS Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abc58b04-06ee-4a85-b9b3-46b67ed41049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/1691399795.py:21: FutureWarning: indices argument is deprecated and will be removed in version 0.20. To avoid this warning, please do not use the indices argument. Please see peak_local_max documentation for more details.\n",
      "  coords = peak_local_max(h, min_distance=1, indices=True)\n",
      "100%|██████████████████████████████████████| 3364/3364 [00:05<00:00, 616.48it/s]\n",
      "100%|██████████████████████████████████████| 4812/4812 [00:05<00:00, 809.35it/s]\n"
     ]
    }
   ],
   "source": [
    "interest_points0, rs0 = ANMS(mosaic1_images[0], n_ip = 500, harris_threshold_scaled = .2, edge_fraction = .02)\n",
    "interest_points1, rs1 = ANMS(mosaic1_images[1], n_ip = 500, harris_threshold_scaled = .2, edge_fraction = .02)\n",
    "\n",
    "show_correspondences([mosaic1_images[0]], [interest_points0], dims = (1, 1))\n",
    "show_correspondences([mosaic1_images[1]], [interest_points1], dims = (1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533e833-3340-4c52-b2e9-e4fc3a0693f0",
   "metadata": {},
   "source": [
    "Feature Descriptor Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f0a8b0e6-1569-4252-8900-cc92a9efad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2228 2256]\n"
     ]
    }
   ],
   "source": [
    "_ = 150\n",
    "show(mosaic1_images[0])\n",
    "print(interest_points0[_])\n",
    "show(feature_descriptor(mosaic1_images[0], np.array([interest_points0[_]])).squeeze(), cmap = 'gray')\n",
    "show(mosaic1_images[0][interest_points0[_][1] - 35//2:interest_points0[_][1] + 35//2, interest_points0[_][0] - 35//2:interest_points0[_][0] + 35//2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95f41e-c378-439d-9656-b286a9ec92aa",
   "metadata": {},
   "source": [
    "Feature Matching Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8aec38cc-5400-472a-bfd9-435675f34cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2228 2256]\n",
      "0.80562406395117 4.8598175048828125\n"
     ]
    }
   ],
   "source": [
    "features1 = feature_descriptor(mosaic1_images[0], interest_points0).reshape(500, -1)\n",
    "features2 = feature_descriptor(mosaic1_images[1], interest_points1).reshape(500, -1)\n",
    "\n",
    "lowe_scores, best_second_best_args, top_scores = feature_matcher(features1, features2)\n",
    "\n",
    "# These should be matches\n",
    "_ = 150\n",
    "show(mosaic1_images[0])\n",
    "print(interest_points0[_])\n",
    "show(features1[_].reshape(8,8), cmap = 'gray')\n",
    "show(features2[best_second_best_args[_][0]].reshape(8,8), cmap = 'gray')\n",
    "\n",
    "# These shouldn't be matches\n",
    "#_ = 150\n",
    "#show(features1[_].reshape(8,8), cmap = 'gray')\n",
    "#show(features2[best_second_best_args[10][0]].reshape(8,8), cmap = 'gray')\n",
    "\n",
    "# This is the lowe score for the top match\n",
    "print(lowe_scores[_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc441107-5ee9-4cc3-b0f8-b6125c962c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 0.28945643609385613\n",
      "[1135 2047]\n"
     ]
    }
   ],
   "source": [
    "# This should demo a matched feature with a low lowe score that is actually overlapped between the two images\n",
    "_ = np.argmin(lowe_scores[200:]) + 200\n",
    "print(_, lowe_scores[_])\n",
    "\n",
    "show(mosaic1_images[0])\n",
    "print(interest_points0[_])\n",
    "\n",
    "show(features1[_].reshape(8,8), cmap = 'gray')\n",
    "show(features2[best_second_best_args[_][0]].reshape(8,8), cmap = 'gray')\n",
    "show(mosaic1_images[0][interest_points0[_][1] - 35//2:interest_points0[_][1] + 35//2, interest_points0[_][0] - 35//2:interest_points0[_][0] + 35//2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28332db-adf2-40dd-bc99-2e55b5fbff61",
   "metadata": {},
   "source": [
    "RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7bf366c-88fb-41c6-a54f-82abce41fae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.ransac(image1, image2, poi_coords=None, iterations=1000, feature_matching_threshold=0.25, inlier_threshold=2, w_size=8, stride=5, edge_fraction=0.02, n_ip=500, min_features=8, debug=False)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10bffe72-d6f6-4df6-b433-32adfc916127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/1691399795.py:21: FutureWarning: indices argument is deprecated and will be removed in version 0.20. To avoid this warning, please do not use the indices argument. Please see peak_local_max documentation for more details.\n",
      "  coords = peak_local_max(h, min_distance=1, indices=True)\n",
      "100%|██████████████████████████████████████| 9026/9026 [00:10<00:00, 892.70it/s]\n",
      "100%|███████████████████████████████████| 16644/16644 [00:09<00:00, 1727.53it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:03<00:00, 3276.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 out of 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#h0, _ = ANMS(images[0], n_ip = 500, harris_threshold_scaled = .1)\n",
    "#h1, _ = ANMS(images[1], n_ip = 500, harris_threshold_scaled = .1)\n",
    "H, keypoints12, inliers, matches = ransac(mosaic1_images[0], mosaic1_images[1], n_ip = 2000,\n",
    "                                                    feature_matching_threshold = .8, iterations = 10000, inlier_threshold=1,\n",
    "                                                    w_size = 8, stride = 5)\n",
    "print(str(len(inliers)) + ' out of ' + str(len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c1afcf20-5592-4905-a5a2-c168a88fdd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "show(fast_warp_image(mosaic1_images[0], H))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bb9ec-de23-45f3-8c9f-580903fe0860",
   "metadata": {},
   "source": [
    "Auto Mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "636a9911-402f-4445-873b-5ce7b2ec3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANSACing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [00:23<00:00, 2108.07it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:25<00:00, 1942.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching\n"
     ]
    }
   ],
   "source": [
    "#interest_points1 = []\n",
    "#for image in mosaic1_images:\n",
    "#    interest_points1.append(ANMS(image, n_ip = 2000, harris_threshold_scaled = .1, edge_fraction = .02)[0])\n",
    "_output1, _images1, _keypoint_pairs1 = auto_mosaic_ordered(mosaic1_images, padding = [(3000, 3000), (1000, 1000)], interest_points = interest_points1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e963d81-54cd-427f-85fc-4114a4ba357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANSACing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [01:41<00:00, 1961.64it/s]\n",
      "100%|█████████████████████████████████| 200000/200000 [01:22<00:00, 2426.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching\n"
     ]
    }
   ],
   "source": [
    "#interest_points2 = []\n",
    "#for image in mosaic2_images:\n",
    "#    interest_points2.append(ANMS(image, n_ip = 2000, harris_threshold_scaled = .1, edge_fraction = .02)[0])\n",
    "_output2, _images2, _keypoint_pairs2 = auto_mosaic_ordered(mosaic2_images, padding = [(3000, 3000), (1000, 1000)], interest_points = interest_points2, iterations = 200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "685937c8-86c2-4b7d-9bd6-f71817e1b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/1691399795.py:21: FutureWarning: indices argument is deprecated and will be removed in version 0.20. To avoid this warning, please do not use the indices argument. Please see peak_local_max documentation for more details.\n",
      "  coords = peak_local_max(h, min_distance=1, indices=True)\n",
      "100%|██████████████████████████████████████| 1098/1098 [00:01<00:00, 983.82it/s]\n",
      "100%|█████████████████████████████████████| 1924/1924 [00:01<00:00, 1521.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANSACing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [01:15<00:00, 2640.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching\n"
     ]
    }
   ],
   "source": [
    "mosaic3_images = [get_image(path)/256 for path in ('im1_3.jpg', 'im2_3.jpg', 'im3_3.jpg', 'im4_3.jpg', 'im5_3.jpg')]\n",
    "mosaic3_images[0] = cv2.resize(mosaic3_images[0], (mosaic3_images[0].shape[1] //2, mosaic3_images[0].shape[0]//2), interpolation = cv2.INTER_AREA)\n",
    "mosaic3_images[1] = cv2.resize(mosaic3_images[1], (mosaic3_images[1].shape[1] //2, mosaic3_images[1].shape[0]//2), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "interest_points3 = [[],[]]\n",
    "interest_points3[0] = ANMS(mosaic3_images[0], n_ip = 500, harris_threshold_scaled = 0.05, edge_fraction = .02)[0]\n",
    "interest_points3[1] = ANMS(mosaic3_images[1], n_ip = 500, harris_threshold_scaled = 0.009, edge_fraction = .02)[0]\n",
    "#interest_points3[2] = ANMS(mosaic3_images[2], n_ip = 2000, harris_threshold_scaled = 0.009, edge_fraction = .02)[0]\n",
    "\n",
    "\n",
    "_output3, _images3, _keypoint_pairs3 = auto_mosaic_ordered(mosaic3_images[:2], padding = [(3000, 3000), (1000, 1000)], interest_points = interest_points3[:2], iterations = 200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "445fa871-2047-44ba-86ac-ce10a27baeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correspondences([mosaic3_images[0]], [interest_points3[0]], dims = (1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b5b7fcf4-43c3-4e5d-86ca-d86b77b92815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "show(_output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8d0dfa57-06ab-4376-a002-686b0fa5b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "18a34b04-7f45-4923-a9de-c332fe862e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(_output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043c961-d7f2-4059-aec3-43a7227c6599",
   "metadata": {},
   "source": [
    "# Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67aa5cd-e137-4e71-8e70-71777afce574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeH(im1_pts, im2_pts):\n",
    "    \"\"\"\n",
    "    Computes the homography from im1 to im2\n",
    "    \n",
    "    [         ]       [         ]\n",
    "    [ im2_pts ] = H @ [ im1_pts ]\n",
    "    [ 1 1 1 1 ]       [ 1 1 1 1 ]\n",
    "    \n",
    "    im1_pts = list of shape (pts, 2)\n",
    "    im2_pts = list of shape (pts, 2)\n",
    "    \"\"\"\n",
    "    b = np.array([[im2_pts[i][0]] for i in range(len(im2_pts))] + \n",
    "                  [[im2_pts[i][1]] for i in range(len(im2_pts))])\n",
    "    \n",
    "    # Get the first half the rows of A\n",
    "    # x, y, 1, 0, 0, 0, -x*x_hat, -y*x_hat\n",
    "    A_1 = [[im1_pts[i][0], im1_pts[i][1], 1, 0, 0, 0,\n",
    "            -im1_pts[i][0]*im2_pts[i][0], -im1_pts[i][1]*im2_pts[i][0]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    # Get the secong half the rows of A\n",
    "    # 0, 0, 0, x, y, 1, -x*y_hat, -y*y_hat\n",
    "    A_2 = [[0, 0, 0, im1_pts[i][0], im1_pts[i][1], 1,\n",
    "            -im1_pts[i][0]*im2_pts[i][1], -im1_pts[i][1]*im2_pts[i][1]] \n",
    "           for i in range(len(im1_pts))]\n",
    "    \n",
    "    A = np.array(A_1 + A_2)\n",
    "        \n",
    "    H_flat = np.linalg.lstsq(A, b)[0]\n",
    "    print(np.linalg.lstsq(A, b))\n",
    "    print(H_flat)\n",
    "    H_flat = H_flat.flatten()\n",
    "    # Add in h_33 which is set to 1\n",
    "    H_flat = np.concatenate((H_flat, [1,]))\n",
    "    \n",
    "    H = H_flat.reshape(3, 3)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a25ff2-98bc-4c64-830c-84cd6d4c47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys  = np.meshgrid(range(images[0].shape[0]), range(images[0].shape[1]))\n",
    "ys = ys.astype(np.float32) + 100\n",
    "xs = xs.astype(np.float32) + 100\n",
    "\n",
    "show(cv2.remap(images[0].astype(np.float32), xs, ys, cv2.INTER_LINEAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5289d9-1cbf-4bfe-95ed-ed6c397ed0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afb961b0-8daf-4c17-ae1c-abbec74ec5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kb/gf39c1_j7z573bq8ks8yjj_m0000gn/T/ipykernel_84993/469947474.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98033084-0ca3-4a38-a923-cc3e9f175664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "136d7864-f3a7-4dcb-af51-da465442d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "389b562e-420b-476e-9485-0640ce541950",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca2e71-2601-45ee-8062-752761013ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2(np.array([features[0].flatten()]), np.array([features[0].flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cb8e8-776c-48e6-ae2c-6341bc9c7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e78ba-2168-4c0e-b932-848021829471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
